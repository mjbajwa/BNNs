---
title: "BNNs Hyperparameter Sampling: Comparing Gibbs vs. HMC (NUTS)"
# author: "Jamal Bajwa"
date: "01/28/2021"
output: 
  beamer_presentation:    
    theme: "boxes"
    slide_level: 2
    toc: true
  fontsize: 12 pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center")
```

<!-- # Objective -->

<!-- ## Background -->

<!-- - The parameters (weights and biases) of a Bayesian Neural Network (BNN) are generally assigned to __groups__ -->

<!-- - The prior distributions for parameters in a groups share __common hyperparameters__ -->

<!--   - For example, all weights in the $j^{th}$ layer might be assigned priors: $P(w_{i,j}) = N(0, \sigma_j)$, $P(1/\sigma_j^2) = Gamma(\alpha, \beta)$ -->

<!-- - The hyperparameters are also parameters of the probabilistic model, whose posterior distribution is a pre-requisite for the predictive distribution of target variable on unseen data -->

<!--   - Thus, sampling the posterior distribution of the hyperparameters is arguably as important as the posterior of the low-level parameters (weights, biases) -->

<!-- ## Objective -->

<!-- - For BNNs with normal priors on weights/biases, a gamma distribution is oft-used to sample the precision of the normal distribution in a group -->

<!-- - Neal (1995) pioneered this idea, and used Gibbs sampling for sampling hyperparameters ($P(\sigma | w_{i,j})$ has an analytical form) -->

<!--   - (The scheme couples Gibbs sampling on hyperparameters and Hamiltonian Monte Carlo updates on the weights) -->

<!-- - Our objective is to test if contemporary adaptive HMC methods (such as No-U-Turn sampler) offer a competitive replacement for Gibbs sampling for hyperparameter sampling in BNNs -->

# Overview

## Experimental Overview

Three sampling techniques for parameters/hyperparameters:

- HMC (Centered Parametrization, NUTS for adaptation)
- HMC (Non-Centered Parametrization, NUTS for adaptation)
- FBM (Gibbs sampling for hyperparameters, HMC for all others)

## Centered vs. Non-Centered Parametrization

- Centered: $$f(W | \mu, \tau) = Normal(\mu, \tau^{-0.5})$$
- Non-Centered: $$W = \mu + \tau^{-1/2}W_{n}, f_{W_{n}}(w_{n}) = Normal(0, 1)$$
- Applies to all low-level weights and biases

## Global Assumptions

Architecture:

- 1 hidden layer, 8 hidden units
- tanh activation 
- _parameter groups_: input-hidden weights, hidden-output weights, hidden biases, output bias
- variance of hidden-output weights scaled by number of units in hidden layer, to approach Gaussian Processes as number of units grows

Data:

- from FBM example 
- input dimensions = 1, output dimensions = 1

Hyperparameter Priors (FBM Notation):

- input-hidden weights hyperparameter: 0.05:0.5
- hidden-output weights hyperparameter: 0.05:0.5 
- hidden layer biases hyperparameter: 0.05:0.5
- output bias hyperparameter: 100
- target noise: 0.05:0.5

## Assumptions 

Total iterations: 20,000

Tuning Parameters for NUTS: 

- `adapt_delta`: 0.8 (default = 0.8)
- `max_treedepth`: 10 (default = 10)
- `adapt_gamma`: 0.05 (default = 0.05)

Initial Values:

- `Weight Precision`: 1
- `Biases Precision`: 1
- `Target Noise Precision`: 1
- `Weights`: 0
- `Biases`: 0

__4 separate chains__ were used with the same initializations but different random seeds. 

# Results

## Predictive Uncertainty

![Predictive Quality](../output/combined_2021_02_01_15_50_56/y_vs_x.png){width=250px, height=250px}

## Prediction Median

![Predictive Median](../output/combined_2021_02_01_15_50_56/y_vs_x_predictive_mean_only.png){width=250px, height=250px}

## Hyperparameters: Measurement Noise - Standard Deviation

![Target Noise Standard Deviation](../output/combined_2021_02_01_15_50_56/target_noise_precision.png){width=250px, height=250px}

## Hyperparameters: Inputs-to-Hidden Weights - Standard Deviation

![Inputs-to-Hidden Weights - Standard Deviation](../output/combined_2021_02_01_15_50_56/input_to_hidden_precision.png){width=250px, height=250px}

## Hyperparameters: Hidden-to-Output Weights - Standard Deviation

![Hidden-to-Output Weights - Standard Deviation](../output/combined_2021_02_01_15_50_56/hidden_to_output_precision.png){width=250px, height=250px}

## Hyperparameters: Hidden Unit Biases - Standard Deviation

![Hidden Unit Biases - Standard Deviation](../output/combined_2021_02_01_15_50_56/hidden_bias_precision.png){width=250px, height=250px}

## Parameters: Inputs-to-Hidden Weights

![Inputs-to-Hidden Weights](../output/combined_2021_02_01_15_50_56/input_to_hidden_weights.png){width=250px, height=250px}

## Parameters: Hidden-to-Output Weights

![Hidden-to-Output Weights](../output/combined_2021_02_01_15_50_56/hidden_to_output_weights.png){width=250px, height=250px}

## Parameters: Hidden Units Biases

![Hidden Units Biases](../output/combined_2021_02_01_15_50_56/hidden_unit_biases.png){width=250px, height=250px}

## Parameters: Output Unit Bias

![Output Unit Bias](../output/combined_2021_02_01_15_50_56/output_unit_bias.png){width=250px, height=250px}

## Stepsize Comparison

![Step Size Comparison](../output/combined_2021_02_01_15_50_56/step_size_comparison.png){width=250px, height=250px}

