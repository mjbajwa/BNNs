{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs\n",
    "\n",
    "ITER = 20000\n",
    "CHAINS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    # Load Data\n",
    "\n",
    "    df = pd.read_table(\"../data/rdata\", header=None, delim_whitespace=True)\n",
    "    df.columns = [\"X\", \"Y\"]\n",
    "    df[\"index\"] = np.where(df.index < 100, \"Train\", \"Test\")\n",
    "\n",
    "    # Create train and test\n",
    "\n",
    "    X_train = np.array(df.loc[df[\"index\"] == \"Train\", \"X\"]).reshape(-1, 1)\n",
    "    Y_train = np.array(df.loc[df[\"index\"] == \"Train\", \"Y\"])\n",
    "    X_test = np.array(df.loc[df[\"index\"] == \"Test\", \"X\"]).reshape(-1, 1)\n",
    "    Y_test = np.array(df.loc[df[\"index\"] == \"Test\", \"Y\"])\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_forward(X, params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward pass of the neural network \n",
    "    \"\"\"\n",
    "    \n",
    "    w_ih = tf.reshape(params[0:8], (1, 8))\n",
    "    b_h = tf.reshape(params[8:16], (8,))\n",
    "    w_ho = tf.reshape(params[16:24], (8, 1))\n",
    "    b_o = tf.reshape(params[24], (1, ))\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    net = tf.nn.tanh(tf.matmul(X, w_ih) + b_h)\n",
    "    net = tf.matmul(net, w_ho) + b_o\n",
    "    preds = net[:, 0]\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_unnormalized(params, x, y):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the log posterior density for any given params \"state vector\", and x and y\n",
    "    \n",
    "    \n",
    "    Indexing for reference:\n",
    "    \n",
    "    Low level parameters: \n",
    "    \n",
    "    w_ih = params[0:7]\n",
    "    b_h = params[8:16]\n",
    "    w_ho = params[16:24]\n",
    "    b_o = params[24]\n",
    "    \n",
    "    Hyperparameters:\n",
    "    \n",
    "    w_prec_ih = params[25]\n",
    "    b_prec_h = params[26]\n",
    "    w_prec_ho = params[27]\n",
    "    y_prec = params[28]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    params = tf.reshape(params, shape=(29, ))\n",
    "    params = tf.cast(params, dtype=tf.float32)\n",
    "    \n",
    "    # Hyperparameter definition\n",
    "    \n",
    "    prior_w_prec_ih = tfd.Gamma(concentration=0.025, rate=0.000625)\n",
    "    prior_b_prec_h = tfd.Gamma(concentration=0.025, rate=0.000625)\n",
    "    prior_w_prec_ho = tfd.Gamma(concentration=0.025, rate=0.000625)\n",
    "    prior_y_prec = tfd.Gamma(concentration=0.025, rate=0.000625)\n",
    "\n",
    "    # Prior definition of weights and biases (centered)\n",
    "\n",
    "    prior_w_ih = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros(8, dtype=tf.float32),\n",
    "        scale_diag=tf.ones(8, dtype=tf.float32)*(1/tf.sqrt(params[25]))\n",
    "    )\n",
    "\n",
    "    prior_b_h = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros(8, dtype=tf.float32),\n",
    "        scale_diag=tf.ones(8, dtype=tf.float32)*(1/tf.sqrt(params[26]))\n",
    "    )\n",
    "\n",
    "    prior_w_ho = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros(8, dtype=tf.float32),\n",
    "        scale_diag=tf.ones(8, dtype=tf.float32)*(1/tf.sqrt(params[27]))*(1/np.sqrt(8)), \n",
    "        # Scaling factor for GP limit as n_units -> inf\n",
    "    )\n",
    "\n",
    "    prior_b_o = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros(1, dtype=tf.float32),\n",
    "        scale_diag=tf.ones(1, dtype=tf.float32)*100\n",
    "    )\n",
    "\n",
    "    # Prior definition of weights and biases (non-centered)\n",
    "    \n",
    "    # Likelihood Function definition\n",
    "    \n",
    "    train_predictions = bnn_forward(x, params)\n",
    "    train_predictions = tf.cast(train_predictions, dtype=tf.float32)\n",
    "\n",
    "    likelihood = tfd.MultivariateNormalDiag(\n",
    "        loc=train_predictions, \n",
    "        scale_diag=tf.ones_like(train_predictions)*(1 / tf.sqrt(params[28]))\n",
    "    )\n",
    "\n",
    "    # Calculate log_posterior_probability upto a constant \n",
    "    # (log posterior = log likelihood + log prior - log Z)\n",
    "\n",
    "    log_prior_prob = prior_w_ih.log_prob(params[0:8]) + \\\n",
    "        prior_b_h.log_prob(params[8:16]) + \\\n",
    "        prior_w_ho.log_prob(params[16:24]) + \\\n",
    "        prior_b_o.log_prob(tf.reshape(params[24], (1, 1))) + \\\n",
    "        prior_w_prec_ih.log_prob(tf.reshape(params[25], (1, 1))) + \\\n",
    "        prior_b_prec_h.log_prob(tf.reshape(params[26], (1, 1))) + \\\n",
    "        prior_w_prec_ho.log_prob(tf.reshape(params[27], (1, 1))) + \\\n",
    "        prior_y_prec.log_prob(tf.reshape(params[28], (1, 1)))\n",
    "    \n",
    "    log_likelihood_prob = likelihood.log_prob(y)\n",
    "    \n",
    "    log_posterior_prob = log_prior_prob + tf.cast(log_likelihood_prob, dtype=tf.float32)\n",
    "\n",
    "    return tf.reshape(log_posterior_prob, [])\n",
    "\n",
    "def log_posterior(params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fix X and Y\n",
    "    \"\"\"\n",
    "    \n",
    "    return log_posterior_unnormalized(params, X_train, Y_train)\n",
    "\n",
    "# # Test the log posterior function on random data\n",
    "\n",
    "# test_inputs = tf.random.uniform((29, ), 1, 100, dtype=tf.float32)\n",
    "# temp = log_posterior_unnormalized(test_inputs, X_train, Y_train)\n",
    "\n",
    "# log_posterior(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(experimental_compile=True)\n",
    "def bnn_forward_measurement_noise(X, params):\n",
    "    \"\"\"\n",
    "    Forward pass of the neural network \n",
    "    \"\"\"\n",
    "\n",
    "    predictive_dist = tfd.Normal(loc=tf.cast(bnn_forward(X, params), dtype=tf.float32),\n",
    "                                 scale=1/tf.sqrt(params[28]))\n",
    "\n",
    "    return predictive_dist.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HMC/NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjbajwa/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py:75: UserWarning:\n",
      "\n",
      "TensorFloat-32 matmul/conv are enabled for NVIDIA Ampere+ GPUs. The resulting loss of precision may hinder MCMC convergence. To turn off, run `tf.config.experimental.enable_tensor_float_32_execution(False)`. For more detail, see https://github.com/tensorflow/community/pull/287.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "WARNING:tensorflow:From /home/mjbajwa/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:175: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Acceptance rate: 0.94045\n",
      "2\n",
      "Acceptance rate: 0.93945\n",
      "3\n",
      "Acceptance rate: 0.99075\n",
      "4\n",
      "Acceptance rate: 0.98275\n"
     ]
    }
   ],
   "source": [
    "# def main(args):\n",
    "\n",
    "# Load Data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "Y_train = Y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Y_test = Y_test.astype('float32')\n",
    "\n",
    "# Define MCMC transition kernels (composition step)\n",
    "\n",
    "# HMC\n",
    "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=log_posterior,\n",
    "    step_size=np.float64(5e-3),\n",
    "    num_leapfrog_steps=1000\n",
    ")\n",
    "\n",
    "# NUTS\n",
    "nuts_kernel = tfp.mcmc.NoUTurnSampler(\n",
    "    log_posterior, step_size=0.1, max_tree_depth=10\n",
    ")\n",
    "\n",
    "# Adaptation inside Dual Averaging Method\n",
    "adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "        nuts_kernel,\n",
    "        num_adaptation_steps=int(500),\n",
    "        target_accept_prob=np.float64(.75)\n",
    ")\n",
    "\n",
    "# Define initial state\n",
    "\n",
    "initial_state = tf.concat([tf.zeros((1, 24)),  # initial low-level parameters (fixed at 0...)\n",
    "                           tf.ones((1, 5))],  # hyperparameters (fixed at 1)\n",
    "                          axis=1)\n",
    "\n",
    "# Trace function wrapper - compile using XLA for speed purposes (used to be faster?)\n",
    "\n",
    "@tf.function(experimental_compile=True)\n",
    "def run_chain(initial_state, num_results=2000, num_burnin_steps=500, seed=1):\n",
    "\n",
    "    return tfp.mcmc.sample_chain(\n",
    "        num_results=num_results,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        current_state=initial_state,\n",
    "        kernel=adaptive_kernel,\n",
    "        trace_fn=lambda cs, kr: kr, \n",
    "        seed=seed)\n",
    "\n",
    "# Run all chains\n",
    "\n",
    "all_results = dict()\n",
    "kernel_results = dict()\n",
    "\n",
    "for chain in range(1, CHAINS + 1):\n",
    "    \n",
    "    print(chain)\n",
    "    samples, kernel_results = run_chain(initial_state=initial_state, num_results=ITER, seed=chain*100)\n",
    "    print(\"Acceptance rate:\", kernel_results.inner_results.is_accepted.numpy().mean())\n",
    "    all_results[chain] = samples\n",
    "    # kernel_results[chain] = kernel_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace plots of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all samples into one tensor\n",
    "\n",
    "samples = tf.concat([all_results[1],\n",
    "           all_results[2],\n",
    "           all_results[3],\n",
    "           all_results[4]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "y_train_pred = np.zeros((10, 100, samples.shape[0]))\n",
    "y_test_pred = np.zeros((10, 100, samples.shape[0]))\n",
    "\n",
    "for i in range(0, samples.shape[0]):\n",
    "    y_train_pred[:, :, i] = bnn_forward_measurement_noise(X_train, tf.reshape(samples[i, :, :], (29, )))\n",
    "    \n",
    "for i in range(0, samples.shape[0]):\n",
    "    y_test_pred[:, :, i] = bnn_forward_measurement_noise(X_test, tf.reshape(samples[i, :, :], (29, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a data frame\n",
    "\n",
    "df_predictions_train = pd.DataFrame({\n",
    "    \n",
    "    \"inputs\": X_train.flatten(),\n",
    "    \"targets\": Y_train.flatten(),\n",
    "    \"mean\": y_train_pred[:, :, :].mean(axis=(0,2)),\n",
    "    \"median\": np.quantile(y_train_pred[:, :, :], 0.5, axis=(0,2)),\n",
    "    \"q1\": np.quantile(y_train_pred[:, :, :], 0.01, axis=(0,2)),\n",
    "    \"q10\": np.quantile(y_train_pred[:, :, :], 0.10, axis=(0,2)),\n",
    "    \"q90\": np.quantile(y_train_pred[:, :, :], 0.90, axis=(0,2)),\n",
    "    \"q99\": np.quantile(y_train_pred[:, :, :], 0.99, axis=(0,2)),\n",
    "    \"label\": \"train\"\n",
    "\n",
    "})\n",
    "\n",
    "df_predictions_test = pd.DataFrame({\n",
    "    \n",
    "    \"inputs\": X_test.flatten(),\n",
    "    \"targets\": Y_test.flatten(),\n",
    "    \"mean\": y_test_pred[:, :, :].mean(axis=(0,2)),\n",
    "    \"median\": np.quantile(y_test_pred[:, :, :], 0.5, axis=(0,2)),\n",
    "    \"q1\": np.quantile(y_test_pred[:, :, :], 0.01, axis=(0,2)),\n",
    "    \"q10\": np.quantile(y_test_pred[:, :, :], 0.10, axis=(0,2)),\n",
    "    \"q90\": np.quantile(y_test_pred[:, :, :], 0.90, axis=(0,2)),\n",
    "    \"q99\": np.quantile(y_test_pred[:, :, :], 0.99, axis=(0,2)),\n",
    "    \"label\": \"test\"\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize results for traces\n",
    "\n",
    "traces = samples.numpy().reshape((ITER*4, 29))\n",
    "\n",
    "w_ih_names = [\"w_ih_\" + str(i) for i in range(0,8)]\n",
    "b_h_names = [\"b_h_\" + str(i) for i in range(0, 8)]\n",
    "w_ho_names = [\"w_ho_\" + str(i) for i in range(0, 8)]\n",
    "b_o_names = [\"b_o_\" + str(i) for i in range(1, 2)]\n",
    "\n",
    "df_traces = pd.DataFrame(traces)\n",
    "df_traces.columns = w_ih_names + b_h_names + w_ho_names + b_o_names + [\"W_prec_ih\"] + [\"B_prec_h\"] + [\"W_prec_ho\"] + [\"y_prec\"]\n",
    "df_traces[\"trace\"] = 1\n",
    "\n",
    "df_traces[\"id\"] = df_traces.index\n",
    "\n",
    "df_traces[\"trace\"] = np.where(np.logical_and(df_traces[\"id\"] >= 0, df_traces[\"id\"] < ITER), 1, \n",
    "                                  np.where(np.logical_and(df_traces[\"id\"] >= ITER, df_traces[\"id\"] < 2*ITER), 2, \n",
    "                                      np.where(np.logical_and(df_traces[\"id\"] >= 2*ITER, df_traces[\"id\"] < 3*ITER), 3, \n",
    "                                              np.where(np.logical_and(df_traces[\"id\"] >= 3*ITER, df_traces[\"id\"] < 4*ITER), 4, 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write traces to disk\n",
    "\n",
    "time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "new_dir = os.path.join(\"../output/\", \"tfprob_c_\" + time)\n",
    "os.mkdir(new_dir)\n",
    "df_traces.to_feather(f\"{new_dir}/df_traces.feather\")\n",
    "\n",
    "# Write predictions to disk\n",
    "\n",
    "df_predictions = pd.concat([df_predictions_train, df_predictions_test]).reset_index()\n",
    "df_predictions.drop(f\"index\", axis=1).to_feather(f\"{new_dir}/df_predictions.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/tfprob_c_2021_03_17_15_59_02\n"
     ]
    }
   ],
   "source": [
    "print(new_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
